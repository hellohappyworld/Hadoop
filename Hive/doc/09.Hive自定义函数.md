#### 自定义函数

为什么要自定义函数：
hive的内部函数不能满足业务的需求。
hive提供多个模块的自定义功能。比如：serde、函数、输入输出格式。

常见的自定义函数：
udf：用户自定义函数，user defined function。一对一的输入输出。
udaf：用户自定义聚合函数。user defined aggregation function.多对一的输入输出。
udtf：用户自定义的表生成函数。user defined table_generate funtion.一对多的输入输出。

测试函数的小技巧：
利用oracle的dual表相似的功能。
create table dual (id string);
insert into dual values(' ');

编写udf的方式：
1、继承自UDF,重写evaluate()，允许重载。
2、继承genericUDF，重写initlizer()、getdisplay()、evaluate()。

```
package UDF;

import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.hive.ql.exec.UDF;

public class FirstUDF extends UDF {
    public static String evaluate(String str) {
        //1.检查参数
        if (StringUtils.isEmpty(str)) {
            return null;
        }

        //2.大写转小写
        return str.toLowerCase();
    }
}

```

使用方式：
第一种：（当前session有效）
1、将编写好的udf的jar包上传到服务器上，并添加到hive的class path中。
hive> add jar /root/TestHive.jar;

2、创建一个自定义的临时函数名
hive> create temporary function toLow as 'UDF.FirstUDF';

3、检查函数是否创建成功
show functions;

4、测试函数
select tolow('HELLOWORLD') from dual;

5、确定没有再需要调用该函数的时候可以销毁函数。
drop [temporary] function if exists tolow;

第二种（相当于临时函数）
1、将编译好的jar包上传到服务器
2、vi ./hive-init    (在hive的安装目录下)
add jar /home/hadoop/testData/TestHive.jar;
create temporary function toLow as 'UDF.FirstUDF';

3、启动hive
hive -i ./hive-init

drop temporary function if exists tolow;

第三种：
1、将编译好的jar包上传到服务器
2、在hive的安装下的bin目录下创建一个文件.hiverc
vi bin/.hiverc
add jar /home/hadoop/testData/TestHive.jar;
create temporary function toLow as 'UDF.FirstUDF';

3、启动hive
hive

###案例1：生日转换成年龄

输入：string birthday
输出：int age

```
package UDF;

import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.hive.ql.exec.UDF;

import java.util.Calendar;

/**
 * input:string
 * output:int
 * 计算逻辑：
 * 1.先比较年份，用当前年份减去生日年份，得到一个年龄
 * 2.比较月份，如果当前月份<生日月历，age-1
 * 3.如果月份=生日月份，比较日期，如果当前日期<生日日期，age-1
 */
public class birthday2age extends UDF {
    public static Integer evaluate(String birth) {
        //1.判断参数
        if (StringUtils.isEmpty(birth)) {
            return -1;
        }

        //2.拆分生日，拆出具体的年月日
        String[] births = birth.split("-");
        int oldyear = Integer.parseInt(births[0]);
        int oldmonth = Integer.parseInt(births[1]);
        int oldday = Integer.parseInt(births[2]);

        //3.获取当前时间
        Calendar calendar = Calendar.getInstance();

        //4.获取当前的年月日的值
        int nowyear = calendar.get(Calendar.YEAR);
        int nowmonth = calendar.get(Calendar.MONTH);
        int nowday = calendar.get(Calendar.DAY_OF_MONTH);

        //5.计算年龄
        int age = nowyear - oldyear;

        if (nowmonth < oldmonth) {
            age -= 1;
        } else if (nowmonth == oldmonth && nowday < oldday) {
            age -= 1;
        }

        return age;
    }

    public static void main(String[] args) {
        System.out.println(new birthday2age().evaluate("1994-12-06"));
    }
}

```

```
将jar包添加到hive的class path中：
hive> add jar /home/hadoop/testData/TestHive1.jar;
创建自定义临时函数：
hive> create temporary function birthdayToAge as 'UDF.birthday2age';
查看函数：
show functions;
测试函数:
hive> select birthdayToage('1994-12-06') from dual;
OK
23
hive> select birthdayToage('1994-01-08') from dual;
OK
24
```

###案例2：根据key值找出value值

如：sex=1&hight=180&weight=130&sal=28000

json格式：
{sex:1,hight:180,weight:130,sal:28000}

```
package UDF;

import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.hive.ql.exec.UDF;
import org.codehaus.jettison.json.JSONException;
import org.codehaus.jettison.json.JSONObject;

public class key2Value extends UDF {
    public static String evaluate(String str, String key) throws JSONException {
        //1.判断参数
        if (StringUtils.isEmpty(str) || StringUtils.isEmpty(key)) {
            return null;
        }

        //2.将输入的字符串转换成json串
        String s1 = str.replace("&", ",");
        String s2 = s1.replace("=", ":");
        String s3 = "{" + s2 + "}";

        //3.加载数据
        JSONObject jo = new JSONObject(s3);

        //4.根据key找到value并返回
        return jo.get(key).toString();
    }

    //用于测试
    public static void main(String[] args) throws JSONException {
        System.out.println(new key2Value().evaluate("sex=1&hight=180&weight=130&sal=28000","weight"));
    }
}
```

```
将jar包提交到集群中并配置hive安装包下的hive-init文件：
vi hive-init:
add jar /home/hadoop/testData/TestHive2.jar;
create temporary function keyToValue as 'UDF.key2Value';
重新启动hive-shell:
hive -i ./hive-init
查看函数：
show functions;
测试函数：
hive> select keyToValue('sex=1&hight=180&weight=130&sal=28000','sal') from dual;
OK
28000
```

###案例3：正则表达式解析日志：

解析前：
220.181.108.151 - - [31/Jan/2012:00:02:32 +0800] \"GET /home.php?mod=space&uid=158&do=album&view=me&from=space HTTP/1.1\" 200 8784 \"-\" \"Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)\"

解析后：
220.181.108.151	20120131 120232	GET	/home.php?mod=space&uid=158&do=album&view=me&from=space	HTTP	200	Mozilla

```
package UDF;

import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.hive.ql.exec.UDF;

import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.Date;
import java.util.Locale;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

/**
 * 输入字符串：
 * 220.181.108.151 - - [31/Jan/2012:00:02:32 +0800] \"GET /home.php?mod=space&uid=158&do=album&view=me&from=space HTTP/1.1\" 200 8784 \"-\" \"Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)\"
 * 输出字符串：
 * 220.181.108.151	20120131 120232	GET	/home.php?mod=space&uid=158&do=album&view=me&from=space	HTTP	200	Mozilla
 */
public class logParser extends UDF {
    public static String evaluate(String log) throws ParseException {
        //1.检查输入参数
        if (StringUtils.isEmpty(log)) {
            return null;
        }

        //2.定义一个正则表达式
        String reg = "^([0-9.]+\\d+) - - \\[(.* \\+\\d+)\\] .+(GET|POST) (.+) (HTTP)\\S+ (\\d+) .+\\\"(\\w+).+$";

        //3.构造正则表达式
        Pattern pattern = Pattern.compile(reg);

        //4.构造匹配器
        Matcher matcher = pattern.matcher(log);
        StringBuffer sb = new StringBuffer();

        //5.判断是否匹配上
        if (matcher.find()) {
            int counter = matcher.groupCount();

            for (int i = 1; i <= counter; i++) {
                if (i == 2) {
                    SimpleDateFormat sd1 = new SimpleDateFormat("yyyyMMdd hhmmss");
                    Date d = new SimpleDateFormat("dd/MMM/yyyy:HH:mm:ss Z", Locale.ENGLISH).parse(matcher.group(i));

                    sb.append(sd1.format(d) + "\t");
                } else {
                    sb.append(matcher.group(i) + "\t");
                }
            }
        }

        return sb.toString();
    }

    public static void main(String[] args) throws ParseException {
        System.out.println(new logParser().evaluate("220.181.108.151 - - [31/Jan/2012:00:02:32 +0800] \\\"GET /home.php?mod=space&uid=158&do=album&view=me&from=space HTTP/1.1\\\" 200 8784 \\\"-\\\" \\\"Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)\\\""));
    }
}
```

```
将jar包提交到集群中并配置hive安装包bin目录下的.hive文件：.hiverc
vi .hiverc
add jar /home/hadoop/testData/TestHive3.jar;
create temporary function logParse as 'UDF.logParser';
查看函数：
show functions;
```

### 案例4：Json数据解析UDF开发

有原始json数据如下：
{"movie":"1193","rate":"5","timeStamp":"978300760","uid":"1"}
{"movie":"661","rate":"3","timeStamp":"978302109","uid":"1"}
{"movie":"914","rate":"3","timeStamp":"978301968","uid":"1"}
{"movie":"3408","rate":"4","timeStamp":"978300275","uid":"1"}
{"movie":"2355","rate":"5","timeStamp":"978824291","uid":"1"}
{"movie":"1197","rate":"3","timeStamp":"978302268","uid":"1"}
{"movie":"1287","rate":"5","timeStamp":"978302039","uid":"1"}

最终我要得到一个结果表：
movie	rate	timestamp	uid
1197	3	978302268	1

步骤：
1、创建一个单字段的表，将原始的json数据加载到表中
2、编写java自定义函数代码，利用自定义函数将json串解析成一个字符串，字符串以\t分割。
3、调用自定义函数，将数据转换到一个临时表。
4、使用split函数从临时表将数据拆分成4个字段保存最终的结果表里。

```
创建表：
hive> create table if not exists ratingjson(
    > json string
    > );
加载数据:
hive> load data local inpath '/home/hadoop/testData/rating.json' into table ratingjson;
```

```
package UDF;

import org.apache.commons.lang.StringUtils;
import org.apache.hadoop.hive.ql.exec.UDF;
import parquet.org.codehaus.jackson.map.ObjectMapper;

import java.io.IOException;


public class jsonParser extends UDF {
    public static String evaluate(String json) {
        //1.检查参数
        if (StringUtils.isEmpty(json)) {
            return null;
        }

        ObjectMapper objectMapper = new ObjectMapper();

        //解析json串到javaBean对象
        try {
            MovieRateBean movieRateBean = objectMapper.readValue(json, MovieRateBean.class);
            return movieRateBean.toString();
        } catch (IOException e) {
            e.printStackTrace();
        }

        return null;
    }

    public static void main(String[] args) {
        System.out.println(new jsonParser().evaluate("{\"movie\":\"1193\",\"rate\":\"5\",\"timeStamp\":\"978300760\",\"uid\":\"1\"}"));
    }
}
********************************************************************************************
package UDF;

public class MovieRateBean {
    private String movie;
    private String rate;
    private String timeStamp;
    private String uid;

    public String getMovie() {
        return movie;
    }

    public void setMovie(String movie) {
        this.movie = movie;
    }

    public String getRate() {
        return rate;
    }

    public void setRate(String rate) {
        this.rate = rate;
    }

    public String getTimeStamp() {
        return timeStamp;
    }

    public void setTimeStamp(String timeStamp) {
        this.timeStamp = timeStamp;
    }

    public String getUid() {
        return uid;
    }

    public void setUid(String uid) {
        this.uid = uid;
    }

    @Override
    public String toString() {
        return movie + "\t" + rate + "\t" + timeStamp + "\t" + uid;
    }
}
```

将jar包提交到集群并在hive安装包的bin下创建.hiverc文件

vi .hiverc

add jar /home/hadoop/testData/TestHive4.jar;
create temporary function jsonToTable as 'UDF.jsonParser';

查看自定义临时函数：

show functions;

创建临时表：

create table jsontemp
as
select jsonparser(json) as json from ratingjson;

select * from jsontemp;

局部结果：

1097	3	978160616	17
2613	3	978160792	17
3417	5	978160437	17
2542	4	978160337	17
3418	4	978159568	17
496	3	978159989	17
2549	3	978158536	17
1748	5	978160157	17
3927	4	978160933	17
1240	5	978160490	17

hive> select split(json,'\t')[0] as movie,split(json,'\t')[1] as rate,split(json,'\t')[2] as time,split(json,'\t')[3] as uid from jsontemp limit 10;
OK
1193	5	978300760	1
661	3	978302109	1
914	3	978301968	1
3408	4	978300275	1
2355	5	978824291	1
1197	3	978302268	1
1287	5	978302039	1
2804	5	978300719	1
594	4	978302268	1
919	4	978301368	1

创建用于保存切分后结果的表：

    hive> create table movieration(
    > movie string,
    > rate string,
    > time string,
    > uid string
    > )
    > row format delimited fields terminated by '\t';
插入数据：

    hive> insert into table movieration
    > select split(json,'\t')[0] as movie,split(json,'\t')[1] as rate,split(json,'\t')[2] as time,split(json,'\t')[3] as uid from jsontemp;

hive> select * from movieration limit 10;
OK
1193	5	978300760	1
661	3	978302109	1
914	3	978301968	1
3408	4	978300275	1
2355	5	978824291	1
1197	3	978302268	1
1287	5	978302039	1
2804	5	978300719	1
594	4	978302268	1
919	4	978301368	1

###案例5：Transform实现

Hive的 TRANSFORM 关键字提供了在SQL中调用自写脚本的功能
适合实现Hive中没有的功能又不想写UDF的情况

需求：将电影评分表中的time转换成weekday，转存到另一个表

1.写一个python脚本，实现时间转换功能：

vi weekday.py

```
#!/bin/python
import sys
import datetime

for line in sys.stdin:
  line = line.strip()
  movie, rate, time,uid = line.split('\t')
  weekday = datetime.datetime.fromtimestamp(float(time)).isoweekday()
  print '\t'.join([movie, rate, str(weekday),uid])
```

2.将python脚本加入hive的classpath

hive> add file /home/hadoop/testData/weekday.py；

3.定义一个结果表用来接受转换结果

```
hive> create table u_data_new(
    > movieid int,
    > rating int,
    > weekday int,
    > userid int)
    > row format delimited fields terminated by '\t';
```

4.使用transform+python查询

```
INSERT OVERWRITE TABLE u_data_new
SELECT
  TRANSFORM (movie ,rate, time,uid)
  USING 'python weekday.py'
  AS (movieid, rating, weekday,userid)
FROM movieration;
```

5.hive> select * from u_data_new limit 10;
OK
1193	5	1	1
661	3	1	1
914	3	1	1
3408	4	1	1
2355	5	7	1
1197	3	1	1
1287	5	1	1
2804	5	1	1
594	4	1	1
919	4	1	1